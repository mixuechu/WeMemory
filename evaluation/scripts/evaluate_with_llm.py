#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨LLMè¯„ä¼°æ£€ç´¢è´¨é‡ï¼šåˆ¤æ–­å¬å›ç»“æœçš„è¯­ä¹‰ç›¸å…³æ€§
"""
import sys
sys.path.insert(0, '.')
from test_hybrid_search import *
from pathlib import Path
import json
from collections import defaultdict
import anthropic
import os

def evaluate_result_relevance_with_llm(query: str, result_content: str, query_type: str) -> dict:
    """
    ä½¿ç”¨Claude APIè¯„ä¼°å•ä¸ªå¬å›ç»“æœçš„è¯­ä¹‰ç›¸å…³æ€§

    è¿”å›:
        {
            'relevance_score': 0-10çš„åˆ†æ•°,
            'reasoning': è¯„åˆ†ç†ç”±
        }
    """
    client = anthropic.Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¿¡æ¯æ£€ç´¢è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°ä¸‹é¢çš„æ£€ç´¢ç»“æœä¸æŸ¥è¯¢çš„è¯­ä¹‰ç›¸å…³æ€§ã€‚

æŸ¥è¯¢ç±»å‹: {query_type}
æŸ¥è¯¢: "{query}"

æ£€ç´¢åˆ°çš„å¯¹è¯å†…å®¹:
{result_content}

è¯·ä»ä»¥ä¸‹å‡ ä¸ªç»´åº¦è¯„ä¼°ç›¸å…³æ€§ï¼ˆ0-10åˆ†ï¼‰ï¼š
1. ç›´æ¥åŒ¹é…ï¼šå†…å®¹æ˜¯å¦ç›´æ¥è®¨è®ºæŸ¥è¯¢ä¸»é¢˜
2. è¯­ä¹‰ç›¸å…³ï¼šå†…å®¹æ˜¯å¦æ¶‰åŠç›¸å…³æ¦‚å¿µã€åŒä¹‰è¯ã€ä¸Šä¸‹æ–‡
3. éšå«ç›¸å…³ï¼šå†…å®¹æ˜¯å¦æœ‰éšå«çš„ç›¸å…³æ€§ï¼ˆå¦‚æƒ…ç»ªã€åœºæ™¯ã€äººç‰©å…³ç³»ç­‰ï¼‰
4. ä¿¡æ¯ä»·å€¼ï¼šè¿™æ®µå¯¹è¯å¯¹ç†è§£æŸ¥è¯¢ä¸»é¢˜æ˜¯å¦æœ‰å¸®åŠ©

è¯„åˆ†æ ‡å‡†ï¼š
10åˆ†ï¼šé«˜åº¦ç›¸å…³ï¼Œç›´æ¥å›ç­”æŸ¥è¯¢æˆ–æä¾›å…³é”®ä¿¡æ¯
7-9åˆ†ï¼šç›¸å…³æ€§å¼ºï¼ŒåŒ…å«é‡è¦çš„ç›¸å…³ä¿¡æ¯
4-6åˆ†ï¼šæœ‰ä¸€å®šç›¸å…³æ€§ï¼Œä½†ä¸æ˜¯æ ¸å¿ƒä¿¡æ¯
1-3åˆ†ï¼šå¼±ç›¸å…³ï¼Œä»…æœ‰é—´æ¥è”ç³»
0åˆ†ï¼šå®Œå…¨ä¸ç›¸å…³

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "relevance_score": <0-10çš„æ•´æ•°>,
    "reasoning": "<ç®€çŸ­çš„è¯„åˆ†ç†ç”±ï¼Œè¯´æ˜ä¸ºä»€ä¹ˆç»™è¿™ä¸ªåˆ†æ•°>"
}}
"""

    try:
        message = client.messages.create(
            model="claude-sonnet-4-5",
            max_tokens=500,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )

        response_text = message.content[0].text

        # æå–JSON
        import re
        json_match = re.search(r'\{[^}]+\}', response_text, re.DOTALL)
        if json_match:
            result = json.loads(json_match.group())
            return result
        else:
            # å¤‡ç”¨ï¼šæ‰‹åŠ¨è§£æ
            return {
                'relevance_score': 5,
                'reasoning': 'æ— æ³•è§£æLLMå“åº”'
            }

    except Exception as e:
        print(f"[ERROR] LLMè¯„ä¼°å¤±è´¥: {e}")
        return {
            'relevance_score': 0,
            'reasoning': f'è¯„ä¼°å¤±è´¥: {str(e)}'
        }

def evaluate_weight_with_llm(
    vector_store: HybridVectorStore,
    embedding_client,
    test_queries: dict,
    bm25_weight: float,
    use_llm: bool = True
) -> dict:
    """ä½¿ç”¨LLMè¯„ä¼°å•ä¸ªæƒé‡é…ç½®çš„æ£€ç´¢è´¨é‡"""

    print(f"\n{'='*80}")
    print(f"è¯„ä¼°æƒé‡é…æ¯” BM25:{bm25_weight:.1f} Vector:{1-bm25_weight:.1f}")
    print(f"{'='*80}")

    results = {
        'bm25_weight': bm25_weight,
        'vector_weight': 1 - bm25_weight,
        'queries': [],
        'metrics': {}
    }

    all_relevance_scores = []
    all_top1_scores = []
    all_top3_avg_scores = []

    query_count = 0

    for query_type in ['keyword', 'semantic', 'mixed']:
        for query in test_queries.get(query_type, []):
            query_count += 1
            print(f"\n[{query_count}] æŸ¥è¯¢: {query} (ç±»å‹: {query_type})")

            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_emb = embedding_client.get_embeddings([query])[0]

            # æ··åˆæ£€ç´¢
            search_results = vector_store.hybrid_search(
                query_content_embedding=query_emb,
                query_text=query,
                top_k=5,
                bm25_weight=bm25_weight,
                vector_weight=1-bm25_weight
            )

            if not search_results:
                continue

            # è¯„ä¼°Top-3ç»“æœ
            query_result = {
                'query': query,
                'type': query_type,
                'results': []
            }

            for rank, result in enumerate(search_results[:3], 1):
                content = result['metadata'].get('content_text', '')[:500]

                # ä½¿ç”¨LLMè¯„ä¼°è¯­ä¹‰ç›¸å…³æ€§
                if use_llm:
                    llm_eval = evaluate_result_relevance_with_llm(query, content, query_type)
                    relevance_score = llm_eval['relevance_score']
                    reasoning = llm_eval['reasoning']
                else:
                    # ä¸ä½¿ç”¨LLMæ—¶ï¼Œç”¨æ··åˆåˆ†æ•°ä»£æ›¿
                    relevance_score = result['score'] * 10
                    reasoning = "æœªä½¿ç”¨LLMè¯„ä¼°"

                query_result['results'].append({
                    'rank': rank,
                    'hybrid_score': result['score'],
                    'bm25_score': result['bm25_score'],
                    'vector_score': result['vector_score'],
                    'relevance_score': relevance_score,
                    'reasoning': reasoning,
                    'content_preview': content[:200]
                })

                if rank == 1:
                    all_top1_scores.append(relevance_score)

                all_relevance_scores.append(relevance_score)

                print(f"  Rank {rank}: è¯­ä¹‰ç›¸å…³æ€§={relevance_score}/10, æ··åˆåˆ†={result['score']:.3f}")
                print(f"    ç†ç”±: {reasoning}")

            # è®¡ç®—Top3å¹³å‡åˆ†
            top3_scores = [r['relevance_score'] for r in query_result['results'][:3]]
            if top3_scores:
                all_top3_avg_scores.append(sum(top3_scores) / len(top3_scores))

            results['queries'].append(query_result)

    # è®¡ç®—æŒ‡æ ‡
    if all_relevance_scores:
        results['metrics'] = {
            'avg_relevance_all': sum(all_relevance_scores) / len(all_relevance_scores),
            'avg_relevance_top1': sum(all_top1_scores) / len(all_top1_scores) if all_top1_scores else 0,
            'avg_relevance_top3': sum(all_top3_avg_scores) / len(all_top3_avg_scores) if all_top3_avg_scores else 0,
            'total_evaluations': len(all_relevance_scores)
        }

        print(f"\n{'='*40}")
        print(f"æŒ‡æ ‡æ±‡æ€»:")
        print(f"  å¹³å‡è¯­ä¹‰ç›¸å…³æ€§(æ‰€æœ‰): {results['metrics']['avg_relevance_all']:.2f}/10")
        print(f"  å¹³å‡è¯­ä¹‰ç›¸å…³æ€§(Top1): {results['metrics']['avg_relevance_top1']:.2f}/10")
        print(f"  å¹³å‡è¯­ä¹‰ç›¸å…³æ€§(Top3): {results['metrics']['avg_relevance_top3']:.2f}/10")
        print(f"  è¯„ä¼°æ•°é‡: {results['metrics']['total_evaluations']}")

    return results

def optimize_weights_with_llm(
    conv_files: list,
    weight_candidates: list = [0.3, 0.5, 0.7, 0.9],
    output_file: str = "llm_evaluation_results.json",
    use_llm: bool = True
):
    """ä½¿ç”¨LLMè¯„ä¼°æ‰¾åˆ°æœ€ä½³æƒé‡é…æ¯”"""

    print("="*80)
    print("åŸºäºLLMçš„è¯­ä¹‰ç›¸å…³æ€§è¯„ä¼° - æƒé‡ä¼˜åŒ–")
    print("="*80)

    # åˆå§‹åŒ–
    embedding_client = GoogleEmbeddingClient()

    all_results = {
        'conversations': [],
        'weight_comparison': defaultdict(lambda: {
            'total_relevance': 0,
            'total_top1': 0,
            'total_top3': 0,
            'count': 0
        })
    }

    # ä¸ºæ¯ä¸ªå¯¹è¯æµ‹è¯•
    for conv_file in conv_files:
        print(f"\n{'='*80}")
        print(f"å¤„ç†å¯¹è¯: {conv_file.name}")
        print(f"{'='*80}")

        # 1. åŠ è½½æˆ–ç”Ÿæˆå‘é‡åº“
        pkl_file = conv_file.parent / f"{conv_file.stem}_dual.pkl"

        if not pkl_file.exists():
            print(f"[INFO] ç”ŸæˆåŒå‘é‡...")
            from test_embedding_simple import VectorKnowledgeBasePipeline
            pipeline = VectorKnowledgeBasePipeline()
            pipeline.process_conversation(conv_file)
            pipeline.vector_store.save(str(pkl_file))

        # 2. åŠ è½½å‘é‡åº“
        print(f"[INFO] åŠ è½½å‘é‡åº“...")
        vector_store = HybridVectorStore(dimension=768)
        vector_store.load(str(pkl_file))
        vector_store.build_bm25_index()

        # 3. ç”Ÿæˆæµ‹è¯•æŸ¥è¯¢
        print(f"[INFO] ç”Ÿæˆæµ‹è¯•æŸ¥è¯¢...")
        from optimize_weights import generate_test_queries_from_conversation
        test_queries = generate_test_queries_from_conversation(conv_file)

        print(f"[INFO] æµ‹è¯•æŸ¥è¯¢æ•°é‡:")
        print(f"  - å…³é”®è¯: {len(test_queries['keyword'])}")
        print(f"  - è¯­ä¹‰: {len(test_queries['semantic'])}")
        print(f"  - æ··åˆ: {len(test_queries['mixed'])}")

        # 4. æµ‹è¯•ä¸åŒæƒé‡
        conv_results = {
            'conversation': test_queries['conversation'],
            'file': str(conv_file),
            'weight_tests': []
        }

        for bm25_w in weight_candidates:
            test_result = evaluate_weight_with_llm(
                vector_store, embedding_client, test_queries, bm25_w, use_llm
            )

            conv_results['weight_tests'].append(test_result)

            # ç´¯ç§¯ç»Ÿè®¡
            metrics = test_result['metrics']
            if metrics:
                all_results['weight_comparison'][bm25_w]['total_relevance'] += metrics['avg_relevance_all']
                all_results['weight_comparison'][bm25_w]['total_top1'] += metrics['avg_relevance_top1']
                all_results['weight_comparison'][bm25_w]['total_top3'] += metrics['avg_relevance_top3']
                all_results['weight_comparison'][bm25_w]['count'] += 1

        all_results['conversations'].append(conv_results)

    # 5. æ±‡æ€»æœ€ä½³æƒé‡
    print(f"\n{'='*80}")
    print("æƒé‡ä¼˜åŒ–ç»“æœæ±‡æ€»ï¼ˆåŸºäºLLMè¯­ä¹‰ç›¸å…³æ€§è¯„ä¼°ï¼‰")
    print(f"{'='*80}")

    best_weight = None
    best_score = -1

    print(f"\n{'æƒé‡é…æ¯”':<20} {'å¹³å‡ç›¸å…³æ€§(å…¨éƒ¨)':<20} {'å¹³å‡ç›¸å…³æ€§(Top1)':<20} {'å¹³å‡ç›¸å…³æ€§(Top3)':<20}")
    print("-" * 80)

    for bm25_w in sorted(weight_candidates):
        stats = all_results['weight_comparison'][bm25_w]
        if stats['count'] > 0:
            avg_all = stats['total_relevance'] / stats['count']
            avg_top1 = stats['total_top1'] / stats['count']
            avg_top3 = stats['total_top3'] / stats['count']

            print(f"BM25:{bm25_w:.1f} Vec:{1-bm25_w:.1f}    {avg_all:.2f}/10              {avg_top1:.2f}/10              {avg_top3:.2f}/10")

            # ä½¿ç”¨Top1å¹³å‡åˆ†ä½œä¸ºä¸»è¦æŒ‡æ ‡
            if avg_top1 > best_score:
                best_score = avg_top1
                best_weight = bm25_w

    all_results['best_weight'] = {
        'bm25': best_weight,
        'vector': 1 - best_weight,
        'avg_relevance_top1': best_score
    }

    print(f"\n{'='*80}")
    print(f"ğŸ¯ æœ€ä½³æƒé‡é…æ¯”: BM25:{best_weight:.1f} Vector:{1-best_weight:.1f}")
    print(f"   å¹³å‡è¯­ä¹‰ç›¸å…³æ€§(Top1): {best_score:.2f}/10")
    print(f"{'='*80}")

    # 6. ä¿å­˜ç»“æœ
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2, default=str)

    print(f"\n[INFO] è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    return all_results

def main():
    """ä¸»å‡½æ•°"""

    # æ£€æŸ¥API key
    if not os.getenv('ANTHROPIC_API_KEY'):
        print("[ERROR] è¯·è®¾ç½® ANTHROPIC_API_KEY ç¯å¢ƒå˜é‡")
        print("[INFO] å¯ä»¥åœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ : ANTHROPIC_API_KEY=your_key")
        return

    # é€‰æ‹©æµ‹è¯•å¯¹è¯ï¼ˆä¸ä¹‹å‰ç›¸åŒï¼‰
    test_conversations = [
        Path("chat_data_filtered/alex_li/alex_li.json"),
        Path("chat_data_filtered/å‘¨ä»•è¾¾/å‘¨ä»•è¾¾.json"),
        Path("chat_data_filtered/é»„å¿ƒæ€¡/é»„å¿ƒæ€¡.json"),
        Path("chat_data_filtered/é˜¡é™Œ/é˜¡é™Œ.json"),
    ]

    # æ£€æŸ¥æ–‡ä»¶
    valid_conversations = [c for c in test_conversations if c.exists()]

    if not valid_conversations:
        print("[ERROR] æœªæ‰¾åˆ°æµ‹è¯•å¯¹è¯æ–‡ä»¶")
        return

    print(f"[INFO] å°†æµ‹è¯• {len(valid_conversations)} ä¸ªå¯¹è¯")
    for conv in valid_conversations:
        print(f"  - {conv.parent.name}")

    # æƒé‡å€™é€‰ï¼ˆå‡å°‘åˆ°4ä¸ªä»¥èŠ‚çœAPIè°ƒç”¨ï¼‰
    weight_candidates = [0.3, 0.5, 0.7, 0.9]

    print(f"\n[INFO] å°†æµ‹è¯• {len(weight_candidates)} ç§æƒé‡é…æ¯”")
    print(f"[INFO] ä½¿ç”¨Claude APIè¿›è¡Œè¯­ä¹‰ç›¸å…³æ€§è¯„ä¼°")
    print(f"[WARNING] è¿™å°†æ¶ˆè€—ä¸€å®šçš„APIé…é¢ï¼Œé¢„è®¡è¯„ä¼°æ¬¡æ•°: {len(valid_conversations)} * {len(weight_candidates)} * ~25æŸ¥è¯¢ * 3ç»“æœ = ~{len(valid_conversations) * len(weight_candidates) * 25 * 3} æ¬¡APIè°ƒç”¨")

    # è¿è¡Œä¼˜åŒ–
    results = optimize_weights_with_llm(
        valid_conversations,
        weight_candidates=weight_candidates,
        use_llm=True
    )

if __name__ == "__main__":
    load_dotenv()
    main()
