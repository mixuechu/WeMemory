# 知识图谱构建与使用方案

## 1. 方案概述

基于微信聊天记录构建轻量级知识图谱，实现：
- **实体关系网络**: 人物、事件、地点、组织的关联
- **时序推理**: 基于时间线的事件推理
- **社交网络分析**: 人际关系强度、社群发现
- **上下文增强**: 为向量检索提供结构化补充

**核心理念**: 不追求"全量"知识图谱，而是构建**高价值、可演进**的轻量级图谱。

---

## 2. 整体架构

```
┌─────────────────────────────────────────────────────────────┐
│                     查询层 (Query Layer)                     │
│  - Cypher查询  - 图遍历  - 路径查询  - 子图提取             │
└──────────────────────┬──────────────────────────────────────┘
                       │
┌──────────────────────▼──────────────────────────────────────┐
│                 推理与分析层 (Analytics)                      │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐    │
│  │关系强度  │  │社群发现  │  │路径推理  │  │时序分析  │    │
│  │计算      │  │          │  │          │  │          │    │
│  └──────────┘  └──────────┘  └──────────┘  └──────────┘    │
└──────────────────────┬──────────────────────────────────────┘
                       │
┌──────────────────────▼──────────────────────────────────────┐
│                  图数据库 (Neo4j)                             │
│  ┌────────────────────────────────────────────────────┐     │
│  │  节点 (Nodes)              边 (Relationships)      │     │
│  │  ─────────────             ──────────────────      │     │
│  │  • Person (人物)           • FRIEND_OF             │     │
│  │  • Event (事件)            • PARTICIPATED_IN       │     │
│  │  • Location (地点)         • HAPPENED_AT           │     │
│  │  • Organization (组织)     • WORKS_FOR             │     │
│  │  • Topic (主题)            • DISCUSSED             │     │
│  │  • Conversation (对话)     • BELONGS_TO            │     │
│  └────────────────────────────────────────────────────┘     │
└──────────────────────┬──────────────────────────────────────┘
                       │
┌──────────────────────▼──────────────────────────────────────┐
│              知识抽取层 (Extraction Layer)                    │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐    │
│  │实体识别  │  │关系抽取  │  │事件抽取  │  │共指消解  │    │
│  │(NER)     │  │(RE)      │  │          │  │          │    │
│  └────┬─────┘  └────┬─────┘  └────┬─────┘  └────┬─────┘    │
└───────┼─────────────┼─────────────┼─────────────┼───────────┘
        └─────────────┴─────────────┴─────────────┘
                       │
┌──────────────────────▼──────────────────────────────────────┐
│                原始数据 (1874个对话JSON)                      │
└─────────────────────────────────────────────────────────────┘
```

---

## 3. 图模型设计

### 3.1 核心节点类型

#### Person (人物)
```cypher
(:Person {
    id: "m453301909",
    name: "User（男）",
    nicknames: ["米老鼠", "雪川"],
    platform_id: "m453301909",
    first_seen: timestamp,
    last_seen: timestamp,
    message_count: 12345,
    avatar_url: "https://...",
    # 计算属性
    activity_score: 0.85,
    centrality: 0.72
})
```

#### Conversation (对话)
```cypher
(:Conversation {
    id: "conv_uuid",
    name: "妈",
    type: "private|group",
    platform: "wechat",
    created_at: timestamp,
    message_count: 5678,
    participant_count: 2,
    is_active: true
})
```

#### Event (事件)
```cypher
(:Event {
    id: "event_uuid",
    title: "2023年春节旅行",
    type: "travel|work|social|life",
    start_time: timestamp,
    end_time: timestamp,
    description: "...",
    participants: ["p1", "p2"],
    # 从对话中提取的关键信息
    extracted_from: ["conv1", "conv2"]
})
```

#### Location (地点)
```cypher
(:Location {
    id: "loc_uuid",
    name: "西安",
    type: "city|province|country|poi",
    mentioned_count: 234,
    coordinates: {lat: 34.34, lon: 108.94}  # 如果能推断
})
```

#### Organization (组织)
```cypher
(:Organization {
    id: "org_uuid",
    name: "华为",
    type: "company|school|government",
    mentioned_count: 123
})
```

#### Topic (主题)
```cypher
(:Topic {
    id: "topic_uuid",
    name: "AI技术",
    category: "technology|life|work|...",
    keywords: ["LLM", "ChatGPT", "Agent"],
    message_count: 456
})
```

---

### 3.2 核心关系类型

#### 人际关系
```cypher
// 朋友关系
(:Person)-[:FRIEND_OF {
    strength: 0.85,           // 关系强度（基于互动频率）
    first_interaction: timestamp,
    last_interaction: timestamp,
    message_count: 234,
    shared_conversations: 5,  // 共同群聊数
    interaction_frequency: 0.7  // 归一化互动频率
}]->(:Person)

// 家人关系
(:Person)-[:FAMILY_OF {
    relationship: "mother|father|sibling",
    message_count: 5678
}]->(:Person)

// 同事关系
(:Person)-[:COLLEAGUE_OF {
    organization: "华为",
    start_date: timestamp,
    end_date: timestamp
}]->(:Person)
```

#### 对话参与
```cypher
(:Person)-[:PARTICIPATED_IN {
    role: "owner|admin|member",
    joined_at: timestamp,
    message_count: 123,
    last_active: timestamp
}]->(:Conversation)
```

#### 事件关联
```cypher
(:Person)-[:PARTICIPATED_IN {
    role: "organizer|participant"
}]->(:Event)

(:Event)-[:HAPPENED_AT]->(:Location)

(:Event)-[:DISCUSSED_IN]->(:Conversation)

(:Event)-[:RELATED_TO]->(:Topic)
```

#### 知识关联
```cypher
(:Person)-[:INTERESTED_IN {
    strength: 0.75,
    first_mentioned: timestamp
}]->(:Topic)

(:Person)-[:WORKS_FOR {
    position: "Engineer",
    start_date: timestamp
}]->(:Organization)

(:Person)-[:LIVES_IN]->(:Location)
```

---

### 3.3 图Schema设计

```python
# Neo4j Schema定义
SCHEMA = """
// 节点约束
CREATE CONSTRAINT person_id IF NOT EXISTS FOR (p:Person) REQUIRE p.id IS UNIQUE;
CREATE CONSTRAINT conversation_id IF NOT EXISTS FOR (c:Conversation) REQUIRE c.id IS UNIQUE;
CREATE CONSTRAINT event_id IF NOT EXISTS FOR (e:Event) REQUIRE e.id IS UNIQUE;
CREATE CONSTRAINT location_id IF NOT EXISTS FOR (l:Location) REQUIRE l.id IS UNIQUE;
CREATE CONSTRAINT org_id IF NOT EXISTS FOR (o:Organization) REQUIRE o.id IS UNIQUE;
CREATE CONSTRAINT topic_id IF NOT EXISTS FOR (t:Topic) REQUIRE t.id IS UNIQUE;

// 索引
CREATE INDEX person_name IF NOT EXISTS FOR (p:Person) ON (p.name);
CREATE INDEX conversation_name IF NOT EXISTS FOR (c:Conversation) ON (c.name);
CREATE INDEX event_time IF NOT EXISTS FOR (e:Event) ON (e.start_time);
CREATE INDEX location_name IF NOT EXISTS FOR (l:Location) ON (l.name);

// 全文搜索索引
CREATE FULLTEXT INDEX entity_search IF NOT EXISTS
FOR (n:Person|Organization|Location|Event)
ON EACH [n.name, n.description];
"""
```

---

## 4. 知识抽取流程

### 4.1 实体识别 (NER)

#### 方案选择

| 方案 | 优点 | 缺点 | 推荐场景 |
|------|------|------|----------|
| **LLM-based (GPT-4)** | 准确率高、无需训练 | 成本高、速度慢 | ⭐⭐⭐⭐⭐ 推荐 |
| **spaCy + 自定义规则** | 快速、免费 | 准确率一般 | 快速原型 |
| **BERT-NER微调** | 准确率可控 | 需要标注数据 | 有标注资源 |

#### LLM实体抽取实现

```python
from openai import OpenAI
from pydantic import BaseModel
from typing import List, Literal

class Entity(BaseModel):
    text: str
    type: Literal["PERSON", "LOCATION", "ORGANIZATION", "EVENT", "TOPIC"]
    context: str  # 上下文片段

class EntityExtractionResult(BaseModel):
    entities: List[Entity]

class LLMEntityExtractor:
    def __init__(self, openai_client: OpenAI):
        self.client = openai_client

    def extract_entities(self, text: str, conversation_context: dict) -> List[Entity]:
        """使用LLM提取实体"""

        prompt = f"""
从以下微信聊天记录中提取实体。

对话背景:
- 对话名称: {conversation_context.get('name', '未知')}
- 参与者: {', '.join(conversation_context.get('participants', []))}

聊天内容:
{text}

提取以下类型的实体:
1. PERSON: 人名（包括昵称、代称）
2. LOCATION: 地点（城市、地标、场所）
3. ORGANIZATION: 组织（公司、学校、政府）
4. EVENT: 事件（旅行、会议、活动）
5. TOPIC: 讨论主题（技术、兴趣爱好）

注意:
- 同一实体的不同称呼视为同一个实体
- 提供足够的上下文帮助理解
- 忽略通用词汇（如"朋友"、"那边"）
"""

        response = self.client.beta.chat.completions.parse(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format=EntityExtractionResult
        )

        return response.choices[0].message.parsed.entities

    def extract_from_conversation(
        self,
        messages: List[dict],
        batch_size: int = 50
    ) -> List[Entity]:
        """批量处理对话"""
        all_entities = []

        for i in range(0, len(messages), batch_size):
            batch = messages[i:i+batch_size]

            # 构建文本
            text = "\n".join([
                f"{m['sender']}: {m['content']}"
                for m in batch
                if m.get('type') == 0  # 只处理文本消息
            ])

            if not text.strip():
                continue

            entities = self.extract_entities(
                text,
                conversation_context={
                    'name': messages[0].get('conversation_name'),
                    'participants': list(set(m['sender'] for m in batch))
                }
            )

            all_entities.extend(entities)

        return self._deduplicate_entities(all_entities)

    def _deduplicate_entities(self, entities: List[Entity]) -> List[Entity]:
        """实体去重与合并"""
        # 简单实现：基于文本去重
        seen = {}
        for entity in entities:
            key = (entity.type, entity.text.lower())
            if key not in seen:
                seen[key] = entity
            else:
                # 合并上下文
                seen[key].context += "; " + entity.context

        return list(seen.values())
```

---

### 4.2 关系抽取

```python
from pydantic import BaseModel
from typing import List, Optional

class Relation(BaseModel):
    subject: str
    subject_type: str
    relation: str
    object: str
    object_type: str
    confidence: float
    evidence: str  # 原文证据

class RelationExtractionResult(BaseModel):
    relations: List[Relation]

class LLMRelationExtractor:
    def __init__(self, openai_client: OpenAI):
        self.client = openai_client

    def extract_relations(
        self,
        text: str,
        entities: List[Entity]
    ) -> List[Relation]:
        """从文本和已识别实体中抽取关系"""

        # 构建实体列表
        entity_list = "\n".join([
            f"- {e.text} ({e.type})"
            for e in entities
        ])

        prompt = f"""
从以下聊天记录中提取实体之间的关系。

已识别的实体:
{entity_list}

聊天内容:
{text}

提取以下类型的关系:
1. FRIEND_OF: 朋友关系
2. FAMILY_OF: 家人关系（mother/father/sibling/...）
3. COLLEAGUE_OF: 同事关系
4. WORKS_FOR: 工作于某组织
5. LIVES_IN: 居住在某地
6. PARTICIPATED_IN: 参与某事件
7. INTERESTED_IN: 对某主题感兴趣
8. HAPPENED_AT: 事件发生在某地
9. DISCUSSED: 讨论某主题

返回格式:
- subject: 关系主体
- relation: 关系类型
- object: 关系客体
- confidence: 置信度 (0-1)
- evidence: 原文证据
"""

        response = self.client.beta.chat.completions.parse(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format=RelationExtractionResult
        )

        return response.choices[0].message.parsed.relations
```

---

### 4.3 知识融合与更新

```python
from neo4j import GraphDatabase
from datetime import datetime

class KnowledgeGraphBuilder:
    def __init__(self, neo4j_uri: str, user: str, password: str):
        self.driver = GraphDatabase.driver(neo4j_uri, auth=(user, password))

    def add_entity(self, entity: Entity, source_metadata: dict):
        """添加实体节点"""
        with self.driver.session() as session:
            query = f"""
            MERGE (e:{entity.type} {{text: $text}})
            ON CREATE SET
                e.id = randomUUID(),
                e.name = $text,
                e.first_seen = datetime($timestamp),
                e.mention_count = 1,
                e.sources = [$source]
            ON MATCH SET
                e.mention_count = e.mention_count + 1,
                e.sources = e.sources + $source,
                e.last_seen = datetime($timestamp)
            RETURN e.id as id
            """

            result = session.run(
                query,
                text=entity.text,
                timestamp=source_metadata['timestamp'],
                source=source_metadata['conversation_id']
            )

            return result.single()['id']

    def add_relation(self, relation: Relation, source_metadata: dict):
        """添加关系边"""
        with self.driver.session() as session:
            query = f"""
            MATCH (s:{relation.subject_type} {{text: $subject}})
            MATCH (o:{relation.object_type} {{text: $object}})
            MERGE (s)-[r:{relation.relation}]->(o)
            ON CREATE SET
                r.strength = $confidence,
                r.first_seen = datetime($timestamp),
                r.evidence = [$evidence],
                r.count = 1
            ON MATCH SET
                r.strength = (r.strength + $confidence) / 2,
                r.last_seen = datetime($timestamp),
                r.evidence = r.evidence + $evidence,
                r.count = r.count + 1
            RETURN r
            """

            session.run(
                query,
                subject=relation.subject,
                object=relation.object,
                confidence=relation.confidence,
                timestamp=source_metadata['timestamp'],
                evidence=relation.evidence
            )

    def build_conversation_graph(self, conversation_data: dict):
        """构建单个对话的子图"""
        with self.driver.session() as session:
            # 1. 创建对话节点
            conv_query = """
            MERGE (c:Conversation {id: $conv_id})
            SET c.name = $name,
                c.type = $type,
                c.message_count = $msg_count,
                c.updated_at = datetime()
            RETURN c.id
            """
            session.run(
                conv_query,
                conv_id=conversation_data['id'],
                name=conversation_data['name'],
                type=conversation_data['type'],
                msg_count=len(conversation_data['messages'])
            )

            # 2. 连接参与者
            for participant in conversation_data['participants']:
                participant_query = """
                MERGE (p:Person {id: $person_id})
                SET p.name = $name
                WITH p
                MATCH (c:Conversation {id: $conv_id})
                MERGE (p)-[r:PARTICIPATED_IN]->(c)
                ON CREATE SET
                    r.joined_at = datetime($first_msg_time),
                    r.message_count = $msg_count
                ON MATCH SET
                    r.message_count = $msg_count,
                    r.last_active = datetime($last_msg_time)
                """
                session.run(
                    participant_query,
                    person_id=participant['id'],
                    name=participant['name'],
                    conv_id=conversation_data['id'],
                    first_msg_time=conversation_data['first_message_time'],
                    last_msg_time=conversation_data['last_message_time'],
                    msg_count=participant['message_count']
                )
```

---

## 5. 社交网络分析

### 5.1 关系强度计算

```python
class SocialNetworkAnalyzer:
    def __init__(self, neo4j_driver):
        self.driver = neo4j_driver

    def calculate_relationship_strength(self):
        """计算所有人际关系的强度"""
        with self.driver.session() as session:
            query = """
            // 查找所有人际互动
            MATCH (p1:Person)-[r:PARTICIPATED_IN]->(c:Conversation)<-[r2:PARTICIPATED_IN]-(p2:Person)
            WHERE id(p1) < id(p2)  // 避免重复
            WITH p1, p2,
                 count(DISTINCT c) as shared_conversations,
                 sum(r.message_count + r2.message_count) as total_messages,
                 max(r.last_active) as last_interaction

            // 计算强度分数
            WITH p1, p2, shared_conversations, total_messages, last_interaction,
                 // 归一化因子
                 (total_messages * 1.0 / 1000) as msg_score,  // 消息数归一化
                 (shared_conversations * 1.0 / 10) as conv_score,  // 共同对话归一化
                 // 时间衰减
                 exp(-duration.between(last_interaction, datetime()).days / 365.0) as time_decay

            WITH p1, p2,
                 (msg_score * 0.4 + conv_score * 0.4 + time_decay * 0.2) as strength_score

            // 创建或更新关系
            MERGE (p1)-[r:FRIEND_OF]-(p2)
            SET r.strength = strength_score,
                r.shared_conversations = shared_conversations,
                r.total_messages = total_messages,
                r.last_interaction = last_interaction

            RETURN p1.name, p2.name, strength_score
            ORDER BY strength_score DESC
            LIMIT 100
            """

            results = session.run(query)
            return [(r['p1.name'], r['p2.name'], r['strength_score']) for r in results]

    def find_closest_friends(self, person_id: str, top_k: int = 10):
        """找出某人最亲密的朋友"""
        with self.driver.session() as session:
            query = """
            MATCH (p:Person {id: $person_id})-[r:FRIEND_OF]-(friend:Person)
            RETURN friend.name as name,
                   r.strength as strength,
                   r.shared_conversations as shared_convs,
                   r.total_messages as messages
            ORDER BY r.strength DESC
            LIMIT $top_k
            """

            results = session.run(query, person_id=person_id, top_k=top_k)
            return [dict(r) for r in results]
```

---

### 5.2 社群发现

```python
class CommunityDetection:
    def __init__(self, neo4j_driver):
        self.driver = neo4j_driver

    def detect_communities(self, algorithm: str = 'louvain'):
        """使用图算法检测社群"""
        with self.driver.session() as session:
            # 使用Neo4j GDS (Graph Data Science)
            queries = {
                'louvain': """
                // 1. 创建图投影
                CALL gds.graph.project(
                    'socialNetwork',
                    'Person',
                    {
                        FRIEND_OF: {
                            orientation: 'UNDIRECTED',
                            properties: 'strength'
                        }
                    }
                )

                // 2. 运行Louvain算法
                CALL gds.louvain.stream('socialNetwork', {
                    relationshipWeightProperty: 'strength'
                })
                YIELD nodeId, communityId
                WITH gds.util.asNode(nodeId) AS person, communityId

                // 3. 保存社群标签
                SET person.community_id = communityId

                RETURN communityId, collect(person.name) as members, count(*) as size
                ORDER BY size DESC
                """,

                'label_propagation': """
                CALL gds.labelPropagation.stream('socialNetwork')
                YIELD nodeId, communityId
                WITH gds.util.asNode(nodeId) AS person, communityId
                SET person.community_id = communityId
                RETURN communityId, collect(person.name) as members, count(*) as size
                ORDER BY size DESC
                """
            }

            results = session.run(queries[algorithm])
            return [dict(r) for r in results]

    def analyze_community(self, community_id: int):
        """分析某个社群的特征"""
        with self.driver.session() as session:
            query = """
            MATCH (p:Person {community_id: $community_id})
            OPTIONAL MATCH (p)-[:PARTICIPATED_IN]->(c:Conversation)
            OPTIONAL MATCH (p)-[:INTERESTED_IN]->(t:Topic)

            WITH p,
                 collect(DISTINCT c.name) as conversations,
                 collect(DISTINCT t.name) as topics

            RETURN p.name as person,
                   conversations,
                   topics,
                   size(conversations) as conv_count,
                   size(topics) as topic_count

            ORDER BY conv_count DESC
            """

            results = session.run(query, community_id=community_id)
            return [dict(r) for r in results]
```

---

### 5.3 中心性分析

```python
def calculate_centrality(self):
    """计算节点中心性"""
    with self.driver.session() as session:
        # PageRank中心性
        pagerank_query = """
        CALL gds.pageRank.stream('socialNetwork')
        YIELD nodeId, score
        WITH gds.util.asNode(nodeId) AS person, score
        SET person.pagerank = score
        RETURN person.name, score
        ORDER BY score DESC
        LIMIT 20
        """

        # Betweenness中心性（中介中心性）
        betweenness_query = """
        CALL gds.betweenness.stream('socialNetwork')
        YIELD nodeId, score
        WITH gds.util.asNode(nodeId) AS person, score
        SET person.betweenness = score
        RETURN person.name, score
        ORDER BY score DESC
        LIMIT 20
        """

        pagerank_results = session.run(pagerank_query)
        betweenness_results = session.run(betweenness_query)

        return {
            'pagerank': [dict(r) for r in pagerank_results],
            'betweenness': [dict(r) for r in betweenness_results]
        }
```

---

## 6. 图查询与应用

### 6.1 典型查询场景

#### 场景1: 找出两个人的共同好友

```cypher
// 查询"我"和"张三"的共同好友
MATCH (me:Person {name: "User（男）"})-[:FRIEND_OF]-(mutual)-[:FRIEND_OF]-(friend:Person {name: "张三"})
WHERE me <> friend
RETURN mutual.name as common_friend,
       size((mutual)-[:PARTICIPATED_IN]->()) as mutual_conversations
ORDER BY mutual_conversations DESC
```

#### 场景2: 追溯某个事件的讨论轨迹

```cypher
// 查询某个事件在哪些对话中被讨论
MATCH (e:Event {title: "春节旅行"})-[:DISCUSSED_IN]->(c:Conversation)<-[:PARTICIPATED_IN]-(p:Person)
WITH e, c, collect(p.name) as participants,
     [(e)-[:HAPPENED_AT]->(l:Location) | l.name][0] as location
RETURN c.name as conversation,
       participants,
       location,
       e.start_time as event_time
ORDER BY event_time DESC
```

#### 场景3: 发现某人的兴趣演变

```cypher
// 查询某人不同时期讨论的主题
MATCH (p:Person {name: "User（男）"})-[:PARTICIPATED_IN]->(c:Conversation)-[:DISCUSSED]->(t:Topic)
WITH p, t, c.created_at as time
ORDER BY time
RETURN t.name as topic,
       collect(DISTINCT datetime(time).year) as years,
       count(*) as mention_count
ORDER BY years[0], mention_count DESC
```

#### 场景4: 社交路径查询

```cypher
// 找出"我"和某人的最短社交路径
MATCH path = shortestPath(
    (me:Person {name: "User（男）"})-[:FRIEND_OF*]-(target:Person {name: "目标人物"})
)
RETURN [node in nodes(path) | node.name] as path,
       length(path) as degree_of_separation
```

#### 场景5: 时间范围内的活跃主题

```cypher
// 查询2023年最活跃的讨论主题
MATCH (t:Topic)<-[:DISCUSSED]-(c:Conversation)
WHERE datetime(c.created_at).year = 2023
WITH t, count(c) as discussion_count
RETURN t.name as topic,
       t.category as category,
       discussion_count
ORDER BY discussion_count DESC
LIMIT 20
```

---

### 6.2 图增强的检索

```python
class GraphEnhancedRetriever:
    """结合知识图谱的检索器"""

    def __init__(self, vector_retriever, neo4j_driver):
        self.vector_retriever = vector_retriever
        self.driver = neo4j_driver

    def search_with_graph_context(
        self,
        query: str,
        top_k: int = 10
    ) -> List[dict]:
        """图增强检索"""

        # 1. 向量检索
        vector_results = self.vector_retriever.search(query, top_k=top_k)

        # 2. 从查询中提取实体
        entities = self._extract_entities_from_query(query)

        # 3. 图谱召回相关上下文
        graph_context = self._get_graph_context(entities)

        # 4. 融合结果
        enhanced_results = self._merge_results(
            vector_results,
            graph_context
        )

        return enhanced_results

    def _get_graph_context(self, entities: List[str]) -> dict:
        """从图谱中获取实体相关上下文"""
        with self.driver.session() as session:
            query = """
            UNWIND $entities as entity_name
            MATCH (e {name: entity_name})

            // 获取相关节点
            OPTIONAL MATCH (e)-[r]-(related)
            WITH e, type(r) as rel_type, collect(related.name) as related_entities

            RETURN e.name as entity,
                   labels(e)[0] as entity_type,
                   collect({relation: rel_type, entities: related_entities}) as connections
            """

            results = session.run(query, entities=entities)
            return [dict(r) for r in results]

    def find_related_conversations(
        self,
        entity_name: str,
        entity_type: str,
        limit: int = 5
    ) -> List[str]:
        """找出提及某实体的对话"""
        with self.driver.session() as session:
            query = f"""
            MATCH (e:{entity_type} {{name: $entity_name}})-[:DISCUSSED_IN]->(c:Conversation)
            RETURN c.id as conversation_id,
                   c.name as conversation_name,
                   c.message_count as messages
            ORDER BY messages DESC
            LIMIT $limit
            """

            results = session.run(
                query,
                entity_name=entity_name,
                limit=limit
            )
            return [r['conversation_id'] for r in results]
```

---

## 7. 增量更新策略

### 7.1 新数据摄入

```python
class IncrementalGraphUpdater:
    def __init__(self, kg_builder, entity_extractor, relation_extractor):
        self.kg_builder = kg_builder
        self.entity_extractor = entity_extractor
        self.relation_extractor = relation_extractor

    def update_with_new_messages(
        self,
        conversation_id: str,
        new_messages: List[dict]
    ):
        """处理新消息并更新图谱"""

        # 1. 实体抽取
        entities = self.entity_extractor.extract_from_conversation(new_messages)

        # 2. 关系抽取
        text = "\n".join([m['content'] for m in new_messages])
        relations = self.relation_extractor.extract_relations(text, entities)

        # 3. 更新图谱
        for entity in entities:
            self.kg_builder.add_entity(
                entity,
                source_metadata={
                    'conversation_id': conversation_id,
                    'timestamp': new_messages[-1]['timestamp']
                }
            )

        for relation in relations:
            self.kg_builder.add_relation(
                relation,
                source_metadata={
                    'conversation_id': conversation_id,
                    'timestamp': new_messages[-1]['timestamp']
                }
            )

        # 4. 重新计算关系强度
        self._recalculate_metrics(conversation_id)

    def _recalculate_metrics(self, conversation_id: str):
        """重新计算相关指标"""
        with self.kg_builder.driver.session() as session:
            # 更新参与度
            session.run("""
                MATCH (p:Person)-[r:PARTICIPATED_IN]->(c:Conversation {id: $conv_id})
                SET r.last_active = datetime(),
                    r.message_count = r.message_count + 1
            """, conv_id=conversation_id)
```

---

## 8. 可视化与分析

### 8.1 社交网络可视化

```python
import networkx as nx
import matplotlib.pyplot as plt
from pyvis.network import Network

class GraphVisualizer:
    def __init__(self, neo4j_driver):
        self.driver = neo4j_driver

    def visualize_social_network(
        self,
        output_file: str = "social_network.html",
        min_strength: float = 0.5
    ):
        """可视化社交网络"""
        with self.driver.session() as session:
            query = """
            MATCH (p1:Person)-[r:FRIEND_OF]-(p2:Person)
            WHERE r.strength >= $min_strength AND id(p1) < id(p2)
            RETURN p1.name as source,
                   p2.name as target,
                   r.strength as weight,
                   p1.community_id as source_community,
                   p2.community_id as target_community
            """

            results = session.run(query, min_strength=min_strength)

            # 使用pyvis创建交互式网络图
            net = Network(height="750px", width="100%", bgcolor="#222222", font_color="white")

            for record in results:
                net.add_node(
                    record['source'],
                    title=record['source'],
                    group=record['source_community']
                )
                net.add_node(
                    record['target'],
                    title=record['target'],
                    group=record['target_community']
                )
                net.add_edge(
                    record['source'],
                    record['target'],
                    value=record['weight'] * 10  # 边的粗细
                )

            net.show(output_file)

    def export_to_networkx(self) -> nx.Graph:
        """导出为NetworkX图对象"""
        with self.driver.session() as session:
            query = """
            MATCH (p1:Person)-[r:FRIEND_OF]-(p2:Person)
            WHERE id(p1) < id(p2)
            RETURN p1.name as source, p2.name as target, r.strength as weight
            """

            results = session.run(query)

            G = nx.Graph()
            for record in results:
                G.add_edge(
                    record['source'],
                    record['target'],
                    weight=record['weight']
                )

            return G
```

---

## 9. 部署与运维

### 9.1 Neo4j部署

```yaml
# docker-compose.yml
version: '3.8'
services:
  neo4j:
    image: neo4j:5.15-community
    ports:
      - "7474:7474"  # HTTP
      - "7687:7687"  # Bolt
    volumes:
      - ./neo4j_data:/data
      - ./neo4j_logs:/logs
      - ./neo4j_import:/var/lib/neo4j/import
      - ./neo4j_plugins:/plugins
    environment:
      - NEO4J_AUTH=neo4j/your_password
      - NEO4J_PLUGINS=["graph-data-science", "apoc"]
      - NEO4J_dbms_memory_heap_max__size=4G
      - NEO4J_dbms_memory_pagecache_size=2G
    restart: unless-stopped
```

### 9.2 数据备份

```python
import subprocess
from datetime import datetime

def backup_neo4j_database(backup_dir: str):
    """备份Neo4j数据库"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_name = f"neo4j_backup_{timestamp}"

    command = [
        "docker", "exec", "neo4j",
        "neo4j-admin", "database", "dump",
        "--database=neo4j",
        f"--to-path=/backups/{backup_name}"
    ]

    subprocess.run(command, check=True)
    print(f"Backup created: {backup_name}")
```

---

## 10. 性能优化

### 10.1 查询优化

```cypher
// 使用索引
CREATE INDEX person_name IF NOT EXISTS FOR (p:Person) ON (p.name);

// 使用参数化查询
// 好的做法
MATCH (p:Person {name: $person_name})
RETURN p

// 避免
MATCH (p:Person)
WHERE p.name = $person_name  // 可能不使用索引
RETURN p

// 限制遍历深度
MATCH path = (p1:Person)-[:FRIEND_OF*..3]-(p2:Person)
WHERE p1.name = $name
RETURN path
LIMIT 100
```

### 10.2 批量导入优化

```python
def batch_import_nodes(nodes: List[dict], batch_size: int = 1000):
    """批量导入节点"""
    with driver.session() as session:
        for i in range(0, len(nodes), batch_size):
            batch = nodes[i:i+batch_size]

            query = """
            UNWIND $nodes as node
            MERGE (p:Person {id: node.id})
            SET p += node.properties
            """

            session.run(query, nodes=batch)
```

---

## 11. 评估指标

### 11.1 图谱质量评估

```python
class GraphQualityMetrics:
    def __init__(self, neo4j_driver):
        self.driver = neo4j_driver

    def calculate_metrics(self) -> dict:
        """计算图谱质量指标"""
        with self.driver.session() as session:
            metrics = {}

            # 1. 节点和边数量
            metrics['node_count'] = session.run(
                "MATCH (n) RETURN count(n) as count"
            ).single()['count']

            metrics['edge_count'] = session.run(
                "MATCH ()-[r]->() RETURN count(r) as count"
            ).single()['count']

            # 2. 平均度数
            metrics['avg_degree'] = session.run("""
                MATCH (n)
                WITH n, size((n)--()) as degree
                RETURN avg(degree) as avg_degree
            """).single()['avg_degree']

            # 3. 连通性
            metrics['connected_components'] = session.run("""
                CALL gds.wcc.stats('socialNetwork')
                YIELD componentCount
                RETURN componentCount
            """).single()['componentCount']

            # 4. 图密度
            total_possible_edges = metrics['node_count'] * (metrics['node_count'] - 1) / 2
            metrics['density'] = metrics['edge_count'] / total_possible_edges if total_possible_edges > 0 else 0

            return metrics
```

---

## 12. 应用场景

### 12.1 个人记忆助手

```python
def answer_personal_question(query: str) -> str:
    """回答个人历史问题"""

    # 示例：谁是我最好的朋友？
    if "最好的朋友" in query:
        friends = analyzer.find_closest_friends("m453301909", top_k=5)
        return f"根据聊天记录，您最亲密的朋友是：{', '.join([f['name'] for f in friends[:3]])}"

    # 示例：我去年去过哪些地方？
    if "去过" in query and "地方" in query:
        locations = query_locations_by_year(2023)
        return f"2023年您去过：{', '.join(locations)}"
```

### 12.2 关系维护提醒

```python
def suggest_relationship_maintenance():
    """建议维护的关系"""
    with driver.session() as session:
        query = """
        MATCH (me:Person {id: $my_id})-[r:FRIEND_OF]-(friend:Person)
        WHERE duration.between(r.last_interaction, datetime()).days > 90
        AND r.strength > 0.6
        RETURN friend.name as name,
               r.last_interaction as last_contact,
               r.strength as relationship_strength
        ORDER BY r.strength DESC
        LIMIT 10
        """

        results = session.run(query, my_id="m453301909")
        return [
            f"{r['name']} - 已{(datetime.now() - r['last_contact']).days}天未联系"
            for r in results
        ]
```

---

## 13. 下一步行动

- [ ] 部署Neo4j环境
- [ ] 实现实体抽取pipeline
- [ ] 构建初始图谱（从核心对话开始）
- [ ] 开发图查询API
- [ ] 与向量知识库集成
- [ ] 建立图谱可视化界面

---

*最后更新: 2026-02-24*
