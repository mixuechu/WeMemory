# 向量知识库设计方案 v2.0 - 个人助理优化版

## 1. 设计理念

### 核心目标
为个人助理Agent提供**高质量、可检索**的记忆系统，支持：
- 自然语言查询历史
- 上下文感知的信息召回
- 时间、人物、事件的关联检索

### 关键洞察
❌ **不要单条消息embed** - "好的"、"哈哈"没有语义
✅ **会话级聚合 + 上下文增强** - 让每个embedding都是完整的记忆片段

---

## 2. Chunk设计方案

### 2.1 会话窗口分割策略

```python
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import List

@dataclass
class Message:
    sender: str
    sender_name: str
    timestamp: datetime
    content: str
    msg_type: int

@dataclass
class ConversationSession:
    """会话片段 - 基本embedding单位"""
    session_id: str
    conversation_name: str
    conversation_type: str  # 'private' or 'group'
    participants: List[str]
    start_time: datetime
    end_time: datetime
    messages: List[Message]

    # 生成的增强内容
    enriched_text: str  # 用于embedding的文本
    summary: str  # LLM生成的摘要
    topics: List[str]  # 提取的主题标签

class SessionBuilder:
    """会话片段构建器"""

    def __init__(
        self,
        time_gap_minutes: int = 30,  # 超过30分钟算新会话
        min_messages: int = 3,       # 最少3条消息
        max_messages: int = 20       # 最多20条消息
    ):
        self.time_gap = timedelta(minutes=time_gap_minutes)
        self.min_messages = min_messages
        self.max_messages = max_messages

    def split_into_sessions(self, messages: List[Message]) -> List[ConversationSession]:
        """将消息流切分为会话片段"""
        sessions = []
        current_batch = []

        for msg in messages:
            # 跳过非文本消息
            if msg.msg_type != 0:  # 0 = 文本
                continue

            # 检查是否需要开始新会话
            if current_batch:
                time_since_last = msg.timestamp - current_batch[-1].timestamp

                # 条件：时间间隔过大 或 消息数达到上限
                if time_since_last > self.time_gap or len(current_batch) >= self.max_messages:
                    # 保存当前会话
                    if len(current_batch) >= self.min_messages:
                        sessions.append(self._build_session(current_batch))
                    current_batch = []

            current_batch.append(msg)

        # 处理最后一批
        if len(current_batch) >= self.min_messages:
            sessions.append(self._build_session(current_batch))

        return sessions

    def _build_session(self, messages: List[Message]) -> ConversationSession:
        """构建会话片段对象"""
        # 这里只是框架，enriched_text在下一步生成
        return ConversationSession(
            session_id=f"{messages[0].timestamp.timestamp()}",
            conversation_name="待填充",
            conversation_type="待填充",
            participants=list(set(m.sender_name for m in messages)),
            start_time=messages[0].timestamp,
            end_time=messages[-1].timestamp,
            messages=messages,
            enriched_text="",  # 下一步生成
            summary="",
            topics=[]
        )
```

### 2.2 会话片段示例

**原始消息**:
```
2023-03-15 10:30:15 | 张三: 周末去爬山吗？
2023-03-15 10:31:22 | 我: 好啊，去哪？
2023-03-15 10:32:08 | 张三: 香山怎么样
2023-03-15 10:32:45 | 我: 行，周六早上8点见
2023-03-15 10:33:12 | 张三: OK，不见不散
```

**切分结果**:
- 时间跨度：3分钟
- 消息数：5条
- 判定：一个完整会话片段 ✅

---

## 3. 文本增强策略（核心）

### 3.1 结构化文本模板

```python
class TextEnricher:
    """文本增强器 - 生成用于embedding的富文本"""

    def enrich_session(self, session: ConversationSession, conv_meta: dict) -> str:
        """生成增强文本"""

        # 格式化时间
        time_str = self._format_time(session.start_time)

        # 格式化对话内容
        dialogue = self._format_dialogue(session.messages)

        # 构建结构化文本
        enriched = f"""
【对话信息】
对话名称: {conv_meta['name']}
对话类型: {self._translate_type(conv_meta['type'])}
时间: {time_str}
参与者: {', '.join(session.participants)}

【对话内容】
{dialogue}

【元数据】
消息数: {len(session.messages)}
时长: {self._calc_duration(session)}
""".strip()

        return enriched

    def _format_time(self, dt: datetime) -> str:
        """人类可读的时间格式"""
        # 2023年3月15日 星期三 上午10:30
        weekday = ['周一', '周二', '周三', '周四', '周五', '周六', '周日'][dt.weekday()]
        hour_period = '上午' if dt.hour < 12 else '下午' if dt.hour < 18 else '晚上'

        return f"{dt.year}年{dt.month}月{dt.day}日 {weekday} {hour_period}{dt.hour}:{dt.minute:02d}"

    def _format_dialogue(self, messages: List[Message]) -> str:
        """格式化对话"""
        lines = []
        for msg in messages:
            # 简化名称（去掉括号等）
            name = msg.sender_name.split('(')[0].split('（')[0].strip()
            lines.append(f"{name}: {msg.content}")
        return '\n'.join(lines)

    def _translate_type(self, conv_type: str) -> str:
        return '私聊' if conv_type == 'private' else '群聊'

    def _calc_duration(self, session: ConversationSession) -> str:
        """计算时长"""
        delta = session.end_time - session.start_time
        minutes = int(delta.total_seconds() / 60)
        if minutes < 1:
            return "不到1分钟"
        elif minutes < 60:
            return f"{minutes}分钟"
        else:
            hours = minutes // 60
            mins = minutes % 60
            return f"{hours}小时{mins}分钟"
```

### 3.2 增强文本示例

```text
【对话信息】
对话名称: 与张三的私聊
对话类型: 私聊
时间: 2023年3月15日 星期三 上午10:30
参与者: 张三, User

【对话内容】
张三: 周末去爬山吗？
User: 好啊，去哪？
张三: 香山怎么样
User: 行，周六早上8点见
张三: OK，不见不散

【元数据】
消息数: 5
时长: 3分钟
```

---

## 4. LLM增强层（可选但推荐）

### 4.1 添加智能摘要

```python
from openai import OpenAI

class LLMEnhancer:
    """使用LLM进一步增强"""

    def __init__(self, openai_client: OpenAI):
        self.client = openai_client

    def add_summary_and_topics(self, enriched_text: str) -> dict:
        """生成摘要和主题标签"""

        prompt = f"""
请分析以下对话，提取关键信息：

{enriched_text}

请提供：
1. 一句话摘要（20字以内）
2. 主题标签（3-5个关键词）
3. 关键实体（人名、地点、时间、事件）

以JSON格式返回：
{{
    "summary": "...",
    "topics": ["...", "..."],
    "entities": {{
        "people": [...],
        "locations": [...],
        "events": [...],
        "time_mentions": [...]
    }}
}}
"""

        response = self.client.chat.completions.create(
            model="gpt-4o-mini",  # 便宜快速
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )

        return json.loads(response.choices[0].message.content)

    def create_final_embedding_text(
        self,
        enriched_text: str,
        llm_result: dict
    ) -> str:
        """组合成最终的embedding文本"""

        return f"""
{enriched_text}

【智能摘要】
{llm_result['summary']}

【主题标签】
{', '.join(llm_result['topics'])}

【关键信息】
人物: {', '.join(llm_result['entities']['people'])}
地点: {', '.join(llm_result['entities']['locations'])}
事件: {', '.join(llm_result['entities']['events'])}
""".strip()
```

### 4.2 最终Embedding文本示例

```text
【对话信息】
对话名称: 与张三的私聊
对话类型: 私聊
时间: 2023年3月15日 星期三 上午10:30
参与者: 张三, User

【对话内容】
张三: 周末去爬山吗？
User: 好啊，去哪？
张三: 香山怎么样
User: 行，周六早上8点见
张三: OK，不见不散

【元数据】
消息数: 5
时长: 3分钟

【智能摘要】
约定周六早上8点去香山爬山

【主题标签】
户外活动, 爬山, 周末计划, 香山, 约定

【关键信息】
人物: 张三, User
地点: 香山
事件: 周末爬山, 周六早上8点见面
```

---

## 5. 存储设计

### 5.1 Qdrant Payload结构

```python
from qdrant_client.models import PointStruct

def create_point(session: ConversationSession, embedding: List[float]) -> PointStruct:
    """创建向量点"""

    return PointStruct(
        id=hash(session.session_id),
        vector=embedding,
        payload={
            # 核心标识
            "session_id": session.session_id,
            "conversation_id": session.conversation_name,
            "conversation_type": session.conversation_type,

            # 时间信息（用于过滤）
            "start_timestamp": int(session.start_time.timestamp()),
            "end_timestamp": int(session.end_time.timestamp()),
            "year": session.start_time.year,
            "month": session.start_time.month,
            "day_of_week": session.start_time.weekday(),
            "hour": session.start_time.hour,

            # 参与者信息
            "participants": session.participants,
            "participant_count": len(session.participants),

            # 统计信息
            "message_count": len(session.messages),
            "duration_minutes": (session.end_time - session.start_time).total_seconds() / 60,

            # 文本内容（用于展示）
            "display_text": session.enriched_text,
            "summary": session.summary,
            "topics": session.topics,

            # 原始消息（便于回溯）
            "original_messages": [
                {
                    "sender": m.sender_name,
                    "content": m.content,
                    "timestamp": int(m.timestamp.timestamp())
                }
                for m in session.messages
            ]
        }
    )
```

---

## 6. 完整Pipeline

```python
class VectorKnowledgeBasePipeline:
    """完整的向量知识库构建流程"""

    def __init__(
        self,
        session_builder: SessionBuilder,
        text_enricher: TextEnricher,
        llm_enhancer: LLMEnhancer,
        embedding_client: OpenAI,
        qdrant_client: QdrantClient
    ):
        self.session_builder = session_builder
        self.text_enricher = text_enricher
        self.llm_enhancer = llm_enhancer
        self.embedding_client = embedding_client
        self.qdrant = qdrant_client

    def process_conversation(
        self,
        conversation_file: Path,
        use_llm_enhancement: bool = True
    ):
        """处理单个对话文件"""

        # 1. 加载对话
        with open(conversation_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        messages = self._parse_messages(data['messages'])
        conv_meta = data['meta']

        # 2. 切分会话片段
        sessions = self.session_builder.split_into_sessions(messages)
        print(f"[INFO] {conv_meta['name']}: 切分为 {len(sessions)} 个会话片段")

        # 3. 处理每个片段
        points = []
        for session in sessions:
            # 3.1 基础文本增强
            session.enriched_text = self.text_enricher.enrich_session(session, conv_meta)
            session.conversation_name = conv_meta['name']
            session.conversation_type = conv_meta['type']

            # 3.2 LLM增强（可选）
            if use_llm_enhancement:
                llm_result = self.llm_enhancer.add_summary_and_topics(session.enriched_text)
                session.summary = llm_result['summary']
                session.topics = llm_result['topics']

                # 更新embedding文本
                embedding_text = self.llm_enhancer.create_final_embedding_text(
                    session.enriched_text,
                    llm_result
                )
            else:
                embedding_text = session.enriched_text

            # 3.3 生成embedding
            embedding = self._get_embedding(embedding_text)

            # 3.4 创建向量点
            point = create_point(session, embedding)
            points.append(point)

        # 4. 批量上传到Qdrant
        if points:
            self.qdrant.upsert(
                collection_name="personal_memory",
                points=points
            )
            print(f"[OK] 已上传 {len(points)} 个向量")

    def _parse_messages(self, raw_messages: List[dict]) -> List[Message]:
        """解析消息"""
        messages = []
        for m in raw_messages:
            messages.append(Message(
                sender=m['sender'],
                sender_name=m['accountName'],
                timestamp=datetime.fromtimestamp(m['timestamp']),
                content=m.get('content', ''),
                msg_type=m['type']
            ))
        return messages

    def _get_embedding(self, text: str) -> List[float]:
        """获取embedding"""
        response = self.embedding_client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return response.data[0].embedding
```

---

## 7. 检索优化

### 7.1 多维度过滤

```python
class SmartRetriever:
    """智能检索器"""

    def search(
        self,
        query: str,
        time_range: Optional[tuple] = None,  # (start, end)
        participants: Optional[List[str]] = None,
        conversation_types: Optional[List[str]] = None,
        top_k: int = 10
    ) -> List[dict]:
        """增强检索"""

        # 构建过滤条件
        filters = []

        if time_range:
            start, end = time_range
            filters.append(
                FieldCondition(
                    key="start_timestamp",
                    range=Range(
                        gte=int(start.timestamp()),
                        lte=int(end.timestamp())
                    )
                )
            )

        if participants:
            filters.append(
                FieldCondition(
                    key="participants",
                    match={"any": participants}
                )
            )

        if conversation_types:
            filters.append(
                FieldCondition(
                    key="conversation_type",
                    match={"any": conversation_types}
                )
            )

        # 执行向量搜索
        query_vector = self._get_embedding(query)

        results = self.qdrant.search(
            collection_name="personal_memory",
            query_vector=query_vector,
            query_filter=Filter(must=filters) if filters else None,
            limit=top_k,
            with_payload=True
        )

        return self._format_results(results)
```

### 7.2 查询示例

```python
# 示例1: "我去年春节去哪玩了？"
retriever.search(
    query="春节旅行 去哪玩",
    time_range=(
        datetime(2023, 1, 1),
        datetime(2023, 3, 1)  # 春节前后
    )
)

# 示例2: "我和张三讨论过什么项目？"
retriever.search(
    query="项目讨论 工作",
    participants=["张三"]
)

# 示例3: "我妈妈的生日是什么时候？"
retriever.search(
    query="妈妈 生日",
    participants=["妈"],
    conversation_types=["private"]
)
```

---

## 8. 成本估算

### 8.1 LLM增强成本（可选）

假设：
- 698个对话
- 平均每个对话10个会话片段
- 总计：6,980个片段

**方案A：不使用LLM增强**
- Embedding成本：6,980 × 200 tokens × $0.02/1M = **$0.28**

**方案B：使用LLM增强**
- Embedding成本：$0.28
- LLM摘要成本：6,980 × 400 tokens × $0.15/1M (gpt-4o-mini) = **$0.42**
- **总计：$0.70**

### 8.2 推荐策略

**阶段1（验证）**:
- 选100个对话测试
- 使用LLM增强
- 评估效果

**阶段2（优化）**:
- 高价值对话（消息多、重要人物）→ LLM增强
- 普通对话 → 仅基础增强
- **混合策略，成本约$0.40**

---

## 9. 实施步骤

### Phase 1: 原型验证（1周）
- [ ] 实现SessionBuilder（会话切分）
- [ ] 实现TextEnricher（基础增强）
- [ ] 选择10个对话测试
- [ ] 验证检索效果

### Phase 2: LLM增强（1周）
- [ ] 实现LLMEnhancer
- [ ] 对比有无LLM增强的效果
- [ ] 优化prompt

### Phase 3: 全量处理（3-5天）
- [ ] 批量处理698个对话
- [ ] 上传到Qdrant
- [ ] 构建检索API

### Phase 4: 优化迭代
- [ ] 调整时间窗口参数
- [ ] 优化过滤策略
- [ ] A/B测试

---

## 10. 关键设计决策总结

| 决策点 | 选择 | 理由 |
|--------|------|------|
| **Chunk粒度** | 会话级（3-20条消息） | 保持语义完整性 |
| **文本增强** | 结构化模板 + 元数据 | 提升检索准确性 |
| **LLM增强** | 可选（推荐高价值对话） | 平衡成本与效果 |
| **时间处理** | 人类可读格式 + 时间戳 | 支持自然查询和精确过滤 |
| **参与者** | 显式包含在文本中 | 支持"我和谁"类查询 |

---

**下一步**: 开始实现原型？

