# 向量知识库存取方案

## 1. 方案概述

基于12GB微信聊天记录构建高性能语义检索系统，实现：
- **语义搜索**: 通过自然语言查询历史对话
- **上下文召回**: 为Agent提供相关历史背景
- **混合检索**: 结合向量相似度和关键词匹配
- **个性化记忆**: 支持多粒度的记忆检索

---

## 2. 整体架构

```
┌─────────────────────────────────────────────────────────┐
│                    查询接口层                            │
│  - 自然语言查询  - 结构化过滤  - 混合检索                │
└──────────────────────┬──────────────────────────────────┘
                       │
┌──────────────────────▼──────────────────────────────────┐
│                  检索编排引擎                            │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐              │
│  │向量召回  │  │关键词召回│  │元数据过滤│              │
│  └────┬─────┘  └────┬─────┘  └────┬─────┘              │
└───────┼─────────────┼─────────────┼────────────────────┘
        │             │             │
┌───────▼─────────────▼─────────────▼────────────────────┐
│                  重排序与融合                            │
│  - RRF融合  - 相关性打分  - 时间衰减  - 去重            │
└──────────────────────┬──────────────────────────────────┘
                       │
┌──────────────────────▼──────────────────────────────────┐
│                   存储层                                 │
│  ┌─────────────────┐  ┌─────────────────┐               │
│  │  向量数据库      │  │  元数据索引     │               │
│  │  (Qdrant)       │  │  (PostgreSQL)   │               │
│  │                 │  │                 │               │
│  │  - Embeddings   │  │  - 时间索引     │               │
│  │  - HNSW索引     │  │  - 人物索引     │               │
│  │  - Payload      │  │  - 主题标签     │               │
│  └─────────────────┘  └─────────────────┘               │
└─────────────────────────────────────────────────────────┘
                       ▲
┌──────────────────────┴──────────────────────────────────┐
│                  数据预处理层                            │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐              │
│  │文本清洗  │  │分块策略  │  │向量化    │              │
│  └──────────┘  └──────────┘  └──────────┘              │
└─────────────────────────────────────────────────────────┘
                       ▲
                       │
              ┌────────┴────────┐
              │  原始聊天记录    │
              │  1874个JSON文件  │
              └──────────────────┘
```

---

## 3. 核心技术方案

### 3.1 数据分块策略 (Chunking Strategy)

#### 方案对比

| 策略 | 描述 | 优点 | 缺点 | 适用场景 |
|------|------|------|------|----------|
| **单消息粒度** | 每条消息独立向量化 | 精确匹配、灵活 | 缺乏上下文 | 短问答、查询特定消息 |
| **对话窗口** | N条消息为一组 | 保留上下文 | 边界割裂 | 连续对话场景 |
| **语义分割** | 按主题自动分组 | 语义完整 | 复杂度高 | 长对话、复杂讨论 |
| **混合分层** | 多粒度并存 | 最佳平衡 | 存储冗余 | **推荐方案** |

#### 推荐方案: 混合分层

```python
# 三层粒度
1. 消息级 (Message Level)
   - 每条文本消息单独向量化
   - 用于精确查找

2. 会话级 (Session Level)
   - 时间相近的消息分组（如30分钟内）
   - 窗口大小: 5-15条消息
   - 用于上下文理解

3. 对话级 (Conversation Level)
   - 每个聊天对象的全局摘要
   - 基于LLM生成的周期性摘要（如按月）
   - 用于宏观检索
```

#### 实现示例

```python
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import List, Optional

@dataclass
class Message:
    id: str
    content: str
    sender: str
    timestamp: datetime
    conversation_id: str

@dataclass
class Chunk:
    chunk_id: str
    level: str  # 'message' | 'session' | 'summary'
    content: str
    messages: List[Message]
    metadata: dict

class ChunkingStrategy:
    def __init__(self, session_time_gap: int = 1800):  # 30分钟
        self.session_time_gap = session_time_gap

    def chunk_messages(self, messages: List[Message]) -> List[Chunk]:
        chunks = []

        # 1. 消息级
        for msg in messages:
            if msg.type == 'text' and len(msg.content) > 10:
                chunks.append(Chunk(
                    chunk_id=f"{msg.id}_msg",
                    level='message',
                    content=msg.content,
                    messages=[msg],
                    metadata={
                        'timestamp': msg.timestamp,
                        'sender': msg.sender,
                        'conversation_id': msg.conversation_id
                    }
                ))

        # 2. 会话级（时间窗口分组）
        sessions = self._group_by_time_window(messages)
        for session in sessions:
            session_text = self._format_session(session)
            chunks.append(Chunk(
                chunk_id=f"{session[0].id}_session",
                level='session',
                content=session_text,
                messages=session,
                metadata={
                    'start_time': session[0].timestamp,
                    'end_time': session[-1].timestamp,
                    'participants': list(set(m.sender for m in session)),
                    'message_count': len(session)
                }
            ))

        return chunks

    def _group_by_time_window(self, messages: List[Message]) -> List[List[Message]]:
        """按时间间隔分组"""
        if not messages:
            return []

        sessions = []
        current_session = [messages[0]]

        for msg in messages[1:]:
            time_gap = (msg.timestamp - current_session[-1].timestamp).seconds

            if time_gap > self.session_time_gap or len(current_session) >= 15:
                sessions.append(current_session)
                current_session = [msg]
            else:
                current_session.append(msg)

        if current_session:
            sessions.append(current_session)

        return sessions

    def _format_session(self, messages: List[Message]) -> str:
        """格式化会话为可读文本"""
        lines = []
        for msg in messages:
            sender_name = msg.sender.split('(')[0]  # 简化名称
            lines.append(f"{sender_name}: {msg.content}")
        return "\n".join(lines)
```

---

### 3.2 Embedding 模型选择

#### 候选方案对比

| 模型 | 维度 | 语言 | 性能 | 成本 | 推荐度 |
|------|------|------|------|------|--------|
| **OpenAI text-embedding-3-large** | 3072 | 多语言 | ⭐⭐⭐⭐⭐ | $0.13/1M tokens | ⭐⭐⭐⭐ |
| **OpenAI text-embedding-3-small** | 1536 | 多语言 | ⭐⭐⭐⭐ | $0.02/1M tokens | ⭐⭐⭐⭐⭐ |
| **BGE-M3** | 1024 | 多语言 | ⭐⭐⭐⭐ | 免费(本地) | ⭐⭐⭐⭐ |
| **m3e-large** | 1024 | 中文 | ⭐⭐⭐⭐ | 免费(本地) | ⭐⭐⭐⭐ |
| **Cohere Embed** | 1024 | 多语言 | ⭐⭐⭐⭐ | $0.10/1M tokens | ⭐⭐⭐ |

#### 推荐方案

**阶段1 (快速验证)**: OpenAI text-embedding-3-small
- 成本低
- 质量好
- 接入简单

**阶段2 (生产环境)**: 混合方案
- 重要对话: OpenAI text-embedding-3-large
- 常规对话: BGE-M3 (本地部署)
- 成本优化: 批量处理 + 缓存

#### 成本估算

```python
# 数据量估算
总消息数 ≈ 1,874 conversations × 平均1000条/conversation = 187万条
文本消息占比 ≈ 60% = 112万条
平均长度 ≈ 50 tokens/条

# 向量化成本（OpenAI small）
总tokens = 1.12M × 50 = 56M tokens
成本 = 56M / 1M × $0.02 = $1.12

# 分层方案
消息级: 112万条 × 50 tokens = 56M tokens → $1.12
会话级: 约20万会话 × 200 tokens = 40M tokens → $0.80
总成本: ~$2 (一次性)
```

---

### 3.3 向量数据库选型

#### 方案对比

| 数据库 | 类型 | 优点 | 缺点 | 推荐场景 |
|--------|------|------|------|----------|
| **Qdrant** | 专用向量DB | 高性能、功能丰富、Rust实现 | 相对年轻 | ⭐⭐⭐⭐⭐ 推荐 |
| **Milvus** | 专用向量DB | 成熟稳定、生态好 | 部署复杂、资源占用大 | 大规模场景 |
| **Chroma** | 轻量向量DB | 简单易用、内置持久化 | 性能一般 | 快速原型 |
| **Weaviate** | 向量搜索引擎 | 功能强大、GraphQL | 学习曲线陡 | 复杂查询 |
| **PG+pgvector** | 扩展插件 | 简单、与PostgreSQL集成 | 性能受限 | 小规模数据 |

#### 推荐: Qdrant

**选择理由**:
1. 性能优秀 (Rust实现)
2. 支持Payload过滤 (元数据筛选)
3. 混合查询能力强
4. 支持多向量 (一个点多个向量)
5. Docker部署简单
6. Python SDK友好

#### 部署配置

```yaml
# docker-compose.yml
version: '3.8'
services:
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"  # REST API
      - "6334:6334"  # gRPC
    volumes:
      - ./qdrant_storage:/qdrant/storage
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
    restart: unless-stopped
```

#### Collection 设计

```python
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PayloadSchemaType

client = QdrantClient(host="localhost", port=6333)

# 创建Collection
client.create_collection(
    collection_name="wechat_messages",
    vectors_config={
        "message": VectorParams(
            size=1536,  # embedding维度
            distance=Distance.COSINE
        ),
        # 支持多向量（可选）
        "session": VectorParams(
            size=1536,
            distance=Distance.COSINE
        )
    }
)

# Payload Schema（自动索引）
client.create_payload_index(
    collection_name="wechat_messages",
    field_name="timestamp",
    field_schema=PayloadSchemaType.INTEGER
)

client.create_payload_index(
    collection_name="wechat_messages",
    field_name="conversation_id",
    field_schema=PayloadSchemaType.KEYWORD
)

client.create_payload_index(
    collection_name="wechat_messages",
    field_name="sender_id",
    field_schema=PayloadSchemaType.KEYWORD
)
```

---

### 3.4 数据插入流程

```python
import hashlib
from openai import OpenAI
from qdrant_client.models import PointStruct

class VectorStore:
    def __init__(self, qdrant_client, openai_client):
        self.qdrant = qdrant_client
        self.openai = openai_client
        self.collection_name = "wechat_messages"

    def ingest_chunks(self, chunks: List[Chunk], batch_size: int = 100):
        """批量插入向量"""
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i+batch_size]

            # 1. 批量生成embeddings
            texts = [c.content for c in batch]
            embeddings = self._get_embeddings(texts)

            # 2. 构建Points
            points = []
            for chunk, embedding in zip(batch, embeddings):
                point = PointStruct(
                    id=self._generate_id(chunk.chunk_id),
                    vector={
                        chunk.level: embedding  # 使用命名向量
                    },
                    payload={
                        "chunk_id": chunk.chunk_id,
                        "level": chunk.level,
                        "content": chunk.content,
                        "timestamp": int(chunk.metadata.get('timestamp', 0).timestamp()),
                        "conversation_id": chunk.metadata.get('conversation_id'),
                        "sender": chunk.metadata.get('sender'),
                        "message_count": chunk.metadata.get('message_count', 1),
                        # 用于展示的元数据
                        "display_text": self._format_display(chunk)
                    }
                )
                points.append(point)

            # 3. 批量上传
            self.qdrant.upsert(
                collection_name=self.collection_name,
                points=points
            )

            print(f"Inserted {len(points)} vectors")

    def _get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """批量获取embeddings"""
        response = self.openai.embeddings.create(
            model="text-embedding-3-small",
            input=texts
        )
        return [e.embedding for e in response.data]

    def _generate_id(self, chunk_id: str) -> str:
        """生成唯一ID（使用hash）"""
        return int(hashlib.md5(chunk_id.encode()).hexdigest()[:16], 16) % (10**10)

    def _format_display(self, chunk: Chunk) -> str:
        """格式化用于显示的文本"""
        if chunk.level == 'message':
            return f"[{chunk.metadata['sender']}]: {chunk.content}"
        elif chunk.level == 'session':
            return f"会话片段 ({chunk.metadata['message_count']}条消息):\n{chunk.content[:200]}..."
        return chunk.content
```

---

### 3.5 检索策略

#### 3.5.1 基础向量检索

```python
from qdrant_client.models import Filter, FieldCondition, Range

class VectorRetriever:
    def __init__(self, qdrant_client, openai_client):
        self.qdrant = qdrant_client
        self.openai = openai_client
        self.collection_name = "wechat_messages"

    def search(
        self,
        query: str,
        top_k: int = 10,
        level: str = "message",  # 'message' or 'session'
        filters: Optional[dict] = None
    ) -> List[dict]:
        """基础向量搜索"""

        # 1. 查询向量化
        query_vector = self._get_embeddings([query])[0]

        # 2. 构建过滤条件
        filter_conditions = self._build_filters(filters)

        # 3. 执行搜索
        results = self.qdrant.search(
            collection_name=self.collection_name,
            query_vector=(level, query_vector),  # 命名向量
            query_filter=filter_conditions,
            limit=top_k,
            with_payload=True
        )

        return [
            {
                "score": r.score,
                "content": r.payload["display_text"],
                "metadata": {
                    "timestamp": r.payload.get("timestamp"),
                    "conversation_id": r.payload.get("conversation_id"),
                    "sender": r.payload.get("sender")
                }
            }
            for r in results
        ]

    def _build_filters(self, filters: Optional[dict]) -> Optional[Filter]:
        """构建Qdrant过滤条件"""
        if not filters:
            return None

        conditions = []

        # 时间范围过滤
        if "start_time" in filters or "end_time" in filters:
            time_filter = {}
            if "start_time" in filters:
                time_filter["gte"] = int(filters["start_time"].timestamp())
            if "end_time" in filters:
                time_filter["lte"] = int(filters["end_time"].timestamp())

            conditions.append(
                FieldCondition(key="timestamp", range=Range(**time_filter))
            )

        # 对话ID过滤
        if "conversation_ids" in filters:
            conditions.append(
                FieldCondition(
                    key="conversation_id",
                    match={"any": filters["conversation_ids"]}
                )
            )

        # 发送者过滤
        if "senders" in filters:
            conditions.append(
                FieldCondition(
                    key="sender",
                    match={"any": filters["senders"]}
                )
            )

        return Filter(must=conditions) if conditions else None
```

#### 3.5.2 混合检索 (Hybrid Search)

```python
from typing import List, Dict
import numpy as np

class HybridRetriever:
    def __init__(self, vector_retriever, keyword_searcher):
        self.vector_retriever = vector_retriever
        self.keyword_searcher = keyword_searcher  # PostgreSQL全文搜索

    def hybrid_search(
        self,
        query: str,
        top_k: int = 20,
        vector_weight: float = 0.7,
        keyword_weight: float = 0.3,
        filters: Optional[dict] = None
    ) -> List[dict]:
        """混合检索：向量 + 关键词"""

        # 1. 向量召回（取2倍数量）
        vector_results = self.vector_retriever.search(
            query=query,
            top_k=top_k * 2,
            filters=filters
        )

        # 2. 关键词召回
        keyword_results = self.keyword_searcher.search(
            query=query,
            top_k=top_k * 2,
            filters=filters
        )

        # 3. RRF融合 (Reciprocal Rank Fusion)
        fused_results = self._rrf_fusion(
            vector_results,
            keyword_results,
            k=60  # RRF常数
        )

        # 4. 时间衰减（可选）
        fused_results = self._apply_time_decay(fused_results)

        # 5. 去重 + TopK
        unique_results = self._deduplicate(fused_results)

        return unique_results[:top_k]

    def _rrf_fusion(
        self,
        vector_results: List[dict],
        keyword_results: List[dict],
        k: int = 60
    ) -> List[dict]:
        """RRF融合算法"""
        scores = {}

        # 向量结果打分
        for rank, result in enumerate(vector_results, 1):
            chunk_id = result['metadata']['chunk_id']
            scores[chunk_id] = scores.get(chunk_id, {
                'rrf_score': 0,
                'data': result
            })
            scores[chunk_id]['rrf_score'] += 1 / (k + rank)

        # 关键词结果打分
        for rank, result in enumerate(keyword_results, 1):
            chunk_id = result['metadata']['chunk_id']
            if chunk_id not in scores:
                scores[chunk_id] = {
                    'rrf_score': 0,
                    'data': result
                }
            scores[chunk_id]['rrf_score'] += 1 / (k + rank)

        # 排序
        ranked = sorted(
            scores.values(),
            key=lambda x: x['rrf_score'],
            reverse=True
        )

        return [item['data'] for item in ranked]

    def _apply_time_decay(
        self,
        results: List[dict],
        decay_factor: float = 0.95,
        half_life_days: int = 365
    ) -> List[dict]:
        """应用时间衰减（近期对话权重更高）"""
        from datetime import datetime

        now = datetime.now().timestamp()

        for result in results:
            timestamp = result['metadata'].get('timestamp', now)
            days_ago = (now - timestamp) / 86400

            # 指数衰减
            time_weight = np.exp(-np.log(2) * days_ago / half_life_days)

            # 调整分数
            original_score = result.get('score', 0.5)
            result['score'] = original_score * (0.7 + 0.3 * time_weight)

        # 重新排序
        results.sort(key=lambda x: x['score'], reverse=True)
        return results

    def _deduplicate(self, results: List[dict]) -> List[dict]:
        """去重（基于chunk_id）"""
        seen = set()
        unique = []

        for result in results:
            chunk_id = result['metadata'].get('chunk_id')
            if chunk_id not in seen:
                seen.add(chunk_id)
                unique.append(result)

        return unique
```

---

### 3.6 高级检索功能

#### 3.6.1 多跳检索（根据初始结果扩展）

```python
class MultiHopRetriever:
    def __init__(self, retriever):
        self.retriever = retriever

    def expand_search(
        self,
        query: str,
        initial_top_k: int = 5,
        expansion_per_result: int = 3
    ) -> List[dict]:
        """多跳检索：基于初始结果扩展相关对话"""

        # 1. 初始检索
        initial_results = self.retriever.search(query, top_k=initial_top_k)

        expanded = initial_results.copy()

        # 2. 对每个结果，检索相同对话的上下文
        for result in initial_results:
            conversation_id = result['metadata']['conversation_id']
            timestamp = result['metadata']['timestamp']

            # 检索同一对话的前后消息
            context_results = self.retriever.search(
                query=query,
                top_k=expansion_per_result,
                filters={
                    "conversation_ids": [conversation_id],
                    "start_time": datetime.fromtimestamp(timestamp - 3600),  # 前1小时
                    "end_time": datetime.fromtimestamp(timestamp + 3600)     # 后1小时
                }
            )

            expanded.extend(context_results)

        # 3. 去重
        return self._deduplicate(expanded)
```

#### 3.6.2 对话式检索（带历史）

```python
class ConversationalRetriever:
    def __init__(self, retriever, llm_client):
        self.retriever = retriever
        self.llm = llm_client
        self.conversation_history = []

    def search_with_history(
        self,
        query: str,
        top_k: int = 10
    ) -> List[dict]:
        """考虑对话历史的检索"""

        # 1. 使用LLM重写查询（融合历史）
        rewritten_query = self._rewrite_query(query)

        # 2. 执行检索
        results = self.retriever.search(
            query=rewritten_query,
            top_k=top_k
        )

        # 3. 更新历史
        self.conversation_history.append({
            "query": query,
            "rewritten": rewritten_query,
            "results_count": len(results)
        })

        return results

    def _rewrite_query(self, query: str) -> str:
        """基于历史重写查询"""
        if not self.conversation_history:
            return query

        history_context = "\n".join([
            f"- {h['query']}"
            for h in self.conversation_history[-3:]  # 最近3轮
        ])

        prompt = f"""
基于对话历史，重写用户查询为更完整的检索语句。

对话历史:
{history_context}

当前查询: {query}

重写后的查询:"""

        response = self.llm.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=100
        )

        return response.choices[0].message.content.strip()
```

---

## 4. 性能优化

### 4.1 批量处理

```python
# 批量embedding（减少API调用）
def batch_embed(texts: List[str], batch_size: int = 2048):
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        response = openai.embeddings.create(
            model="text-embedding-3-small",
            input=batch
        )
        embeddings.extend([e.embedding for e in response.data])
    return embeddings
```

### 4.2 缓存机制

```python
import redis
import json
import hashlib

class EmbeddingCache:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.ttl = 86400 * 30  # 30天

    def get_embedding(self, text: str) -> Optional[List[float]]:
        key = self._make_key(text)
        cached = self.redis.get(key)
        if cached:
            return json.loads(cached)
        return None

    def set_embedding(self, text: str, embedding: List[float]):
        key = self._make_key(text)
        self.redis.setex(
            key,
            self.ttl,
            json.dumps(embedding)
        )

    def _make_key(self, text: str) -> str:
        hash_val = hashlib.sha256(text.encode()).hexdigest()
        return f"embed:v1:{hash_val}"
```

### 4.3 异步处理

```python
import asyncio
from qdrant_client import AsyncQdrantClient

async def async_ingest(chunks: List[Chunk]):
    client = AsyncQdrantClient(host="localhost")

    # 并发获取embeddings
    tasks = [get_embedding_async(c.content) for c in chunks]
    embeddings = await asyncio.gather(*tasks)

    # 批量插入
    await client.upsert(
        collection_name="wechat_messages",
        points=[
            PointStruct(id=i, vector=emb, payload=chunks[i].metadata)
            for i, emb in enumerate(embeddings)
        ]
    )
```

---

## 5. 监控与评估

### 5.1 关键指标

```python
class RetrievalMetrics:
    def __init__(self):
        self.query_log = []

    def log_query(
        self,
        query: str,
        results: List[dict],
        latency_ms: float,
        user_clicked: Optional[int] = None
    ):
        self.query_log.append({
            "query": query,
            "results_count": len(results),
            "latency_ms": latency_ms,
            "top_score": results[0]['score'] if results else 0,
            "user_clicked_rank": user_clicked,
            "timestamp": datetime.now()
        })

    def get_metrics(self) -> dict:
        return {
            "avg_latency": np.mean([q['latency_ms'] for q in self.query_log]),
            "p95_latency": np.percentile([q['latency_ms'] for q in self.query_log], 95),
            "avg_results": np.mean([q['results_count'] for q in self.query_log]),
            "mrr": self._calculate_mrr(),  # Mean Reciprocal Rank
        }

    def _calculate_mrr(self) -> float:
        """计算MRR（用户点击位置的倒数平均值）"""
        reciprocal_ranks = [
            1 / q['user_clicked_rank']
            for q in self.query_log
            if q['user_clicked_rank'] is not None
        ]
        return np.mean(reciprocal_ranks) if reciprocal_ranks else 0
```

### 5.2 A/B测试框架

```python
class ABTestRetriever:
    def __init__(self, retriever_a, retriever_b):
        self.retrievers = {'A': retriever_a, 'B': retriever_b}
        self.results_log = []

    def search(self, query: str, user_id: str, **kwargs):
        # 基于user_id哈希分流
        variant = 'A' if hash(user_id) % 2 == 0 else 'B'

        retriever = self.retrievers[variant]
        results = retriever.search(query, **kwargs)

        self.results_log.append({
            'user_id': user_id,
            'variant': variant,
            'query': query,
            'timestamp': datetime.now()
        })

        return results
```

---

## 6. 部署清单

### 6.1 依赖安装

```bash
# requirements.txt
qdrant-client==1.7.0
openai==1.10.0
redis==5.0.1
numpy==1.24.0
pandas==2.0.0
pydantic==2.5.0
python-dotenv==1.0.0
```

### 6.2 环境变量

```bash
# .env
OPENAI_API_KEY=sk-xxx
QDRANT_HOST=localhost
QDRANT_PORT=6333
REDIS_HOST=localhost
REDIS_PORT=6379
```

### 6.3 初始化脚本

```python
# init_vector_store.py
import asyncio
from pathlib import Path
from vector_store import VectorStore, ChunkingStrategy
from data_loader import load_conversations

async def main():
    # 1. 加载数据
    conversations = load_conversations(Path("./聊天记录"))

    # 2. 分块
    chunker = ChunkingStrategy()
    all_chunks = []
    for conv in conversations:
        chunks = chunker.chunk_messages(conv.messages)
        all_chunks.extend(chunks)

    print(f"Total chunks: {len(all_chunks)}")

    # 3. 向量化并插入
    store = VectorStore()
    await store.ingest_chunks(all_chunks, batch_size=100)

    print("Vector store initialized!")

if __name__ == "__main__":
    asyncio.run(main())
```

---

## 7. 使用示例

```python
from vector_store import HybridRetriever
from datetime import datetime, timedelta

# 初始化检索器
retriever = HybridRetriever(...)

# 1. 简单查询
results = retriever.search("去年春节我去哪玩了?")

# 2. 带时间过滤
results = retriever.search(
    "关于项目的讨论",
    filters={
        "start_time": datetime(2023, 1, 1),
        "end_time": datetime(2023, 12, 31)
    }
)

# 3. 指定对话范围
results = retriever.search(
    "技术方案",
    filters={
        "conversation_ids": ["工作群", "技术讨论组"]
    }
)

# 4. 混合检索
results = retriever.hybrid_search(
    "AI产品设计",
    vector_weight=0.7,
    keyword_weight=0.3
)
```

---

## 8. 下一步计划

- [ ] 完成数据预处理脚本
- [ ] 搭建Qdrant环境
- [ ] 实现基础向量化流程
- [ ] 开发检索API
- [ ] 性能基准测试
- [ ] 与知识图谱集成

---

*最后更新: 2026-02-24*
